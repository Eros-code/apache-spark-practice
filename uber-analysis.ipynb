{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **APACHE SPARK SHOWCASE:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In very simple words Pandas run operations on a single machine whereas PySpark runs on multiple machines. If you are working on a Machine Learning application where you are dealing with larger datasets, PySpark is a best fit which could processes operations many times(100x) faster than Pandas.\n",
    "\n",
    "Pandas uses a single machine to perform iterative processes on a dataset whilst apache spark uses multiple machines.\n",
    "\n",
    "Pandas can load the data by reading CSV, JSON, SQL, many other formats and creates a DataFrame which is a structured object containing rows and columns (similar to SQL table).\n",
    "\n",
    "It doesnâ€™t support distributed processing hence you would always need to increase the resources when you need additional horsepower to support your growing data.\n",
    "\n",
    "PySpark is a Spark library written in Python to run Python applications using Apache Spark capabilities. Using PySpark we can run applications parallelly on the distributed cluster (multiple nodes) or even on a single node."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to Decide Between Pandas vs PySpark:\n",
    "\n",
    "Below are the few considerations when to choose PySpark over Pandas\n",
    "\n",
    "- If your data is huge and grows significantly over the years and you wanted to improve your processing time.\n",
    "\n",
    "- If you want fault-tolerant.\n",
    "\n",
    "- ANSI SQL compatibility.\n",
    "- Language to choose (Spark supports Python, Scala, Java & R)\n",
    "- When you want Machine-learning capability.\n",
    "- Would like to read Parquet, Avro, Hive, Casandra, Snowflake e.t.c\n",
    "- If you wanted to stream the data and process it real-time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing apache spark in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyArrow in /opt/homebrew/lib/python3.10/site-packages (10.0.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/homebrew/lib/python3.10/site-packages (from PyArrow) (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyspark in /opt/homebrew/lib/python3.10/site-packages (3.3.1)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /opt/homebrew/lib/python3.10/site-packages (from pyspark) (0.10.9.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install PyArrow\n",
    "%pip install pyspark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark uses Java underlying hence you need to have Java on your Mac. Since Java is a third party, you can install it using the Homebrew command brew. \n",
    "\n",
    "Since Oracle Java is not open source anymore, I am using the OpenJDK version 11. \n",
    "\n",
    "Run the below command in the terminal to install it.\n",
    "\n",
    "`\n",
    "brew install java`\n",
    "\n",
    "and then enter the following\n",
    "\n",
    "`sudo ln -sfn /opt/homebrew/opt/openjdk/libexec/openjdk.jdk \\\n",
    "     /Library/Java/JavaVirtualMachines/openjdk.jdk`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize a spark session:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SparkSession can be used create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files. To create a SparkSession, use the following builder pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/04 13:04:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.appName('PySpark DataFrame From External Files').getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a dataframe using pyspark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('uber.csv', sep = ',', inferSchema = True, header = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inferSchema option tells the reader to infer data types from the source file. This results in an additional pass over the file resulting in two Spark jobs being triggered. It is an expensive operation because Spark must automatically go through the CSV file and infer the schema for each column."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[index: int, key: timestamp, fare_amount: double, pickup_datetime: timestamp, pickup_longitude: double, pickup_latitude: double, dropoff_longitude: double, dropoff_latitude: double, passenger_count: int]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- key: timestamp (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+-------------------+------------------+-----------------+------------------+-----------------+---------------+\n",
      "|   index|                key|fare_amount|    pickup_datetime|  pickup_longitude|  pickup_latitude| dropoff_longitude| dropoff_latitude|passenger_count|\n",
      "+--------+-------------------+-----------+-------------------+------------------+-----------------+------------------+-----------------+---------------+\n",
      "|24238194|2015-05-07 19:52:06|        7.5|2015-05-07 20:52:06|-73.99981689453125|40.73835372924805|   -73.99951171875|40.72321701049805|              1|\n",
      "|27835199|2009-07-17 20:04:56|        7.7|2009-07-17 21:04:56|        -73.994355|        40.728225|         -73.99471|        40.750325|              1|\n",
      "|44984355|2009-08-24 21:45:00|       12.9|2009-08-24 22:45:00|        -74.005043|         40.74077|        -73.962565|        40.772647|              1|\n",
      "|25894730|2009-06-26 08:22:21|        5.3|2009-06-26 09:22:21|        -73.976124|        40.790844|        -73.965316|        40.803349|              3|\n",
      "|17610152|2014-08-28 17:47:00|       16.0|2014-08-28 18:47:00|        -73.925023|        40.744085|-73.97308199999999|        40.761247|              5|\n",
      "+--------+-------------------+-----------+-------------------+------------------+-----------------+------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   index|\n",
      "+--------+\n",
      "|24238194|\n",
      "|27835199|\n",
      "|44984355|\n",
      "|25894730|\n",
      "|17610152|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('index').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+-------------------+------------------+------------------+------------------+-----------------+---------------+\n",
      "|   index|                key|fare_amount|    pickup_datetime|  pickup_longitude|   pickup_latitude| dropoff_longitude| dropoff_latitude|passenger_count|\n",
      "+--------+-------------------+-----------+-------------------+------------------+------------------+------------------+-----------------+---------------+\n",
      "|19277743|2014-06-04 06:49:00|       39.5|2014-06-04 07:49:00|-73.78808000000001|         40.642187|        -73.865042|        40.725997|              4|\n",
      "|22405517|2013-01-03 22:24:41|       56.8|2013-01-03 22:24:41|        -73.993498|         40.764686|        -73.993498|        40.764686|              1|\n",
      "|25485719|2009-08-07 10:43:07|      49.57|2009-08-07 11:43:07|-73.97505799999999|          40.78882|-73.97505799999999|         40.78882|              1|\n",
      "|37942404|2011-11-18 09:51:00|       30.9|2011-11-18 09:51:00|        -73.995888|         40.759078|-73.86500500000001|        40.770452|              1|\n",
      "|46435788|2015-05-15 18:58:16|       43.0|2015-05-15 19:58:16|-73.86270141601562|40.768959045410156|-73.99909210205078|40.74182891845703|              2|\n",
      "+--------+-------------------+-----------+-------------------+------------------+------------------+------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['fare_amount'] > 30).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also convert the DF as a table which can be queried using SQL programatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+-------------------+------------------+------------------+------------------+------------------+---------------+\n",
      "|   index|                key|fare_amount|    pickup_datetime|  pickup_longitude|   pickup_latitude| dropoff_longitude|  dropoff_latitude|passenger_count|\n",
      "+--------+-------------------+-----------+-------------------+------------------+------------------+------------------+------------------+---------------+\n",
      "|24238194|2015-05-07 19:52:06|        7.5|2015-05-07 20:52:06|-73.99981689453125| 40.73835372924805|   -73.99951171875| 40.72321701049805|              1|\n",
      "|27835199|2009-07-17 20:04:56|        7.7|2009-07-17 21:04:56|        -73.994355|         40.728225|         -73.99471|         40.750325|              1|\n",
      "|44984355|2009-08-24 21:45:00|       12.9|2009-08-24 22:45:00|        -74.005043|          40.74077|        -73.962565|         40.772647|              1|\n",
      "|25894730|2009-06-26 08:22:21|        5.3|2009-06-26 09:22:21|        -73.976124|         40.790844|        -73.965316|         40.803349|              3|\n",
      "|17610152|2014-08-28 17:47:00|       16.0|2014-08-28 18:47:00|        -73.925023|         40.744085|-73.97308199999999|         40.761247|              5|\n",
      "|44470845|2011-02-12 02:27:09|        4.9|2011-02-12 02:27:09|-73.96901899999999|          40.75591|-73.96901899999999|          40.75591|              1|\n",
      "|48725865|2014-10-12 07:04:00|       24.5|2014-10-12 08:04:00|-73.96144699999999|40.693965000000006|        -73.871195|         40.774297|              5|\n",
      "|44195482|2012-12-11 13:52:00|        2.5|2012-12-11 13:52:00|               0.0|               0.0|               0.0|               0.0|              1|\n",
      "|15822268|2012-02-17 09:32:00|        9.7|2012-02-17 09:32:00|        -73.975187|         40.745767|         -74.00272|40.743536999999996|              1|\n",
      "|50611056|2012-03-29 19:06:00|       12.5|2012-03-29 20:06:00|        -74.001065|         40.741787|         -73.96304|         40.775012|              1|\n",
      "| 2205147|2015-05-22 17:32:27|        6.5|2015-05-22 18:32:27| -73.9743881225586| 40.74695205688477|-73.98858642578125|40.729804992675774|              1|\n",
      "| 6379048|2011-05-23 22:15:00|        8.5|2011-05-23 23:15:00|               0.0|               0.0|               0.0|               0.0|              1|\n",
      "|31892535|2011-05-17 14:03:00|        3.3|2011-05-17 15:03:00|        -73.966378|          40.80444|         -73.96589|         40.807133|              5|\n",
      "|13012786|2011-06-25 11:19:00|       10.9|2011-06-25 12:19:00|        -73.953352|         40.767382|         -73.97251|         40.796137|              1|\n",
      "|48411337|2010-04-06 22:20:27|        6.9|2010-04-06 23:20:27|-73.97336999999999|         40.755193|-73.97826500000001|         40.766375|              1|\n",
      "|46272151|2012-02-21 09:33:00|        9.7|2012-02-21 09:33:00|        -73.990718|          40.75192|-73.97305300000001|          40.74423|              2|\n",
      "|11875730|2011-09-01 09:21:40|        4.9|2011-09-01 10:21:40|        -73.988908|         40.756982|        -73.981246|          40.76005|              1|\n",
      "| 1728270|2011-03-19 23:58:27|       10.5|2011-03-19 23:58:27|-74.00566500000001|         40.741138|-73.97783000000001|         40.749338|              2|\n",
      "|49173512|2015-03-25 08:58:35|       12.0|2015-03-25 08:58:35|-73.96253204345702|40.767189025878906|-73.97445678710938| 40.75386047363281|              1|\n",
      "|33157445|2009-08-08 00:20:00|        4.9|2009-08-08 01:20:00|        -73.992075|         40.719633|        -73.985323|         40.727405|              1|\n",
      "+--------+-------------------+-----------+-------------------+------------------+------------------+------------------+------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"mytable\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM mytable\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[index: int, key: timestamp, fare_amount: double, pickup_datetime: timestamp, pickup_longitude: double, pickup_latitude: double, dropoff_longitude: double, dropoff_latitude: double, passenger_count: int]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Count of Null, None, NaN of All DataFrame Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,isnan,when,count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "df_clean = df.dropna()\n",
    "num_empty_rows = df.count() - df_clean.count()\n",
    "\n",
    "print(num_empty_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------+---------------+----------------+---------------+-----------------+----------------+---------------+\n",
      "|index|key|fare_amount|pickup_datetime|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|passenger_count|\n",
      "+-----+---+-----------+---------------+----------------+---------------+-----------------+----------------+---------------+\n",
      "+-----+---+-----------+---------------+----------------+---------------+-----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlDF = spark.sql(\"SELECT * FROM mytable where index = NULL\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Column Based on Another Column of DataFrame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need Latitude and Longitude to calculate the distance between two locations with following \n",
    "\n",
    "formula: = acos(sin(lat1)*sin(lat2)+cos(lat1)*cos(lat2)*cos(lon2-lon1))*6371 (6371 is Earth radius in km.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add column from existing column\n",
    "from pyspark.sql.functions import sin,acos,cos\n",
    "\n",
    "\n",
    "lon_1 = df.pickup_longitude\n",
    "lon_2 = df.dropoff_longitude\n",
    "lat_1 = df.pickup_latitude\n",
    "lat_2 = df.dropoff_latitude\n",
    "\n",
    "df_journey = df.withColumn(\"journey_distance\", acos(sin(lat_1)*sin(lat_2)+cos(lat_1)*cos(lat_2)*cos(lon_2-lon_1))*6371)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|fare_amount|  journey_distance|\n",
      "+-----------+------------------+\n",
      "|        7.5| 96.45539652114111|\n",
      "|        7.7|140.81707713218566|\n",
      "|       12.9|337.58282294402926|\n",
      "|        5.3|105.25932472526593|\n",
      "|       16.0| 324.0006003790506|\n",
      "|        4.9|               0.0|\n",
      "+-----------+------------------+\n",
      "only showing top 6 rows\n",
      "\n",
      "+------------------+------------------+-----------------+-----------------+\n",
      "|  pickup_longitude| dropoff_longitude|  pickup_latitude| dropoff_latitude|\n",
      "+------------------+------------------+-----------------+-----------------+\n",
      "|-73.99981689453125|   -73.99951171875|40.73835372924805|40.72321701049805|\n",
      "|        -73.994355|         -73.99471|        40.728225|        40.750325|\n",
      "|        -74.005043|        -73.962565|         40.74077|        40.772647|\n",
      "|        -73.976124|        -73.965316|        40.790844|        40.803349|\n",
      "|        -73.925023|-73.97308199999999|        40.744085|        40.761247|\n",
      "|-73.96901899999999|-73.96901899999999|         40.75591|         40.75591|\n",
      "+------------------+------------------+-----------------+-----------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_journey.select(df_journey[\"fare_amount\"],df_journey[\"journey_distance\"]).show(6)\n",
    "df_journey.select(df_journey[\"pickup_longitude\"],df_journey[\"dropoff_longitude\"], df_journey[\"pickup_latitude\"],df_journey[\"dropoff_latitude\"]).show(6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing the 6th entry of journey distance to respective longitude and latitude we find something very interesting. Despite the customer being charged 4.9 There was no distance covered in the journey.\n",
    "\n",
    "https://www.ridesharingdriver.com/uber-fees-cancellation-booking-cleaning-fees/\n",
    "\n",
    "upon further investigation on the uber website - When a ride request or current trip is canceled, you may be paid a cancellation fee. Cancellation fees are noted in your payment statement.\n",
    "\n",
    "Cancellation fee: Approximately $5 if you cancel two minutes after requesting a ride or if you take longer than 5 minutes to come out to your driver\n",
    "\n",
    "Thus the reason for the customer being charged may be due to the fact that they have cancelled the request for a ride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Premium Uber rides (Uber Black, Black SUV, Lux)\n",
    "You will be charged a $10 cancellation fee if you cancel more than 5 minutes after a driver accepts your ride request, or if your driver cancels your ride after waiting more than 15 minutes at your pickup location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
